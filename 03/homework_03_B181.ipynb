{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework nr. 3 - features transformation & selection (deadline 22/11/2018)\n",
    "\n",
    "In short, the main task is to play with transformations and feature selection methods in order to obtain the best results for linear regression model predicting house sale prices.\n",
    "  \n",
    "> The instructions are not given in details: It is up to you to come up with ideas on how to fulfill the particular tasks as best you can. ;)\n",
    "\n",
    "## What are you supposed to do\n",
    "\n",
    "Your aim is to optimize the _RMSLE_ (see the note below) of the linear regression estimator (=our prediction model) of the observed sale prices.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "  1. Download the dataset from the course pages (hw3_data.csv, hw3_data_description.txt). It corresponds to [this Kaggle competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques).\n",
    "  2. Split the dataset into train & test part exactly as we did in the tutorial.\n",
    "  3. Transform the features properly (don't forget the target variable).\n",
    "  4. Try to find the best subset of features.\n",
    "  5. Compare your results with the [Kaggle leaderboard](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/leaderboard). You should be able to reach approximately the top 20% there.\n",
    "  \n",
    "Give comments on each step of your solution, with short explanations of your choices.\n",
    "\n",
    "  \n",
    "**Note**: _RMSLE_ is a Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sale prices.\n",
    "\n",
    "\n",
    "## Comments\n",
    "\n",
    "  * Please follow the instructions from https://courses.fit.cvut.cz/MI-PDD/homeworks/index.html.\n",
    "  * If the reviewing teacher is not satisfied, he can give you another chance to rework your homework and to obtain more points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, optimize\n",
    "from sklearn import model_selection, linear_model, metrics, preprocessing, feature_selection\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('hw3_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING\n",
    "# Convert all object values to categorial format\n",
    "df[df.select_dtypes(include=['object']).columns] = df.select_dtypes(include=['object']).apply(pd.Series.astype, dtype='category')\n",
    "# Fill all NaN with 0\n",
    "df.loc[:,df.select_dtypes(include=['float64']).columns] = df.loc[:,df.select_dtypes(include=['float64']).columns].fillna(0)\n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "df[df.select_dtypes(['float16', 'float64', 'int64']).columns] = df[df.select_dtypes(['float16', 'float64', 'int64']).columns].astype('float64')\n",
    "\n",
    "df = df[df.columns[df.min() != df.max()]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalArea'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "\n",
    "for column in df.filter(regex='Area|SF', axis=1).columns: \n",
    "    df['Has' + column] = (df[column] > 0).replace({True: 1, False: 0}).astype('uint8')\n",
    "    df['Sqrt' + column] = np.sqrt(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize \n",
    "columns = df.select_dtypes(include=['float64']).columns \n",
    "columns = columns.drop('SalePrice')\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(df[columns])\n",
    "df[columns] = scaler.transform(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmsle(train,train_price, validate,validate_price):\n",
    "    clf = linear_model.LinearRegression()\n",
    "    model = clf.fit(train, train_price) \n",
    "    res =clf.predict(validate)    \n",
    "    return math.sqrt(metrics.mean_squared_error(validate_price, clf.predict(validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pleton\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3157: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Removing features\n",
    "columns_to_remove = ['Id']\n",
    "ttest_pvals = df\\\n",
    "    .drop(columns_to_remove, axis = 1, errors = 'ignore')\\\n",
    "    .select_dtypes(include = ['uint8']).columns\\\n",
    "    .to_series()\\\n",
    "    .apply(lambda x: stats.ttest_ind(df.SalePrice[df[x] == 0], df.SalePrice[df[x] == 1], equal_var = False).pvalue)\n",
    "\n",
    "columns_to_remove = list(set(columns_to_remove + list(ttest_pvals[ttest_pvals > 0.6].index)))\n",
    "df.drop(columns_to_remove, axis = 1, errors = 'ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log\n",
    "df[\"SalePrice\"] = np.log1p(df[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1095 ; Validation:  365\n"
     ]
    }
   ],
   "source": [
    "#Spliting\n",
    "dt, dv = model_selection.train_test_split(df, test_size=0.25, random_state=17)\n",
    "dt = dt.copy()\n",
    "dv = dv.copy()\n",
    "print('Train: ', len(dt), '; Validation: ', len(dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pleton\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_without = dt.drop('SalePrice', axis=1)\n",
    "dv_without = dv.drop('SalePrice', axis=1)\n",
    "\n",
    "#I have tried many models, but Lasso has the best results.\n",
    "clf = linear_model.Lasso(alpha = 0.001)\n",
    "sfm = feature_selection.SelectFromModel(clf)\n",
    "sfm.fit(dt_without, dt.SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features: 84\n"
     ]
    }
   ],
   "source": [
    "feature_idx = sfm.get_support()\n",
    "feature_name = dt_without.columns[feature_idx]\n",
    "\n",
    "#print(f'Selected features: \\n{list(feature_name)}')\n",
    "print(f'Number of selected features: {len(feature_name)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE: 0.1164806015259141\n"
     ]
    }
   ],
   "source": [
    "X = sfm.transform(dt_without)\n",
    "y = dt.SalePrice\n",
    "Xv = sfm.transform(dv_without)\n",
    "yv = dv.SalePrice\n",
    "\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit(X, y) \n",
    "    \n",
    "rmsle = np.sqrt(metrics.mean_squared_error(clf.predict(Xv),yv))\n",
    "print('RMSLE:', rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have done many solutions of this homework and usage **Lasso model** with **sklearn.feature_selection.SelectFromModel** had the best result. I have selected 84 features and final RMSLE is 0.1165.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
